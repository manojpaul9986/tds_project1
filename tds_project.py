# -*- coding: utf-8 -*-
"""TDS Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yb4v36eU7aq5RRX0JWjXF8eAOWHj7GMm

Scraping GitHub user data from Chicago with over 100 followers, using Python and the GitHub API to capture user and repository details, then processed and saved them in CSV files.
"""

import aiohttp
import asyncio
import csv
import time

# GitHub API setup
TOKEN = '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'  # Replace with your GitHub token
HEADERS = {
    'Authorization': f'token {TOKEN}',
    'Accept': 'application/vnd.github.v3+json'
}

# Async function to fetch data with error handling
async def fetch(session, url):
    async with session.get(url) as response:
        if response.status != 200:
            print(f"Error: {response.status} for URL {url}")
            return {}
        try:
            return await response.json()
        except Exception as e:
            print(f"Failed to decode JSON from URL {url}: {e}")
            return {}

# Get users in Chicago with over 100 followers
async def get_users_in_chicago():
    users = []
    url = 'https://api.github.com/search/users'
    page = 1
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        while True:
            data = await fetch(session, f"{url}?q=location:Chicago+followers:>100&per_page=30&page={page}")
            if 'items' not in data:
                break  # Stop if 'items' key is missing
            users.extend(data['items'])
            if len(data['items']) < 30:
                break
            page += 1
            await asyncio.sleep(0.5)
    return users

# Get repositories for a specific user
async def get_user_repos(session, username):
    url = f'https://api.github.com/users/{username}/repos?per_page=100'
    repos = []
    page = 1
    while True:
        data = await fetch(session, f"{url}&page={page}")
        if not isinstance(data, list):  # Check if data is a list (valid repos response)
            break
        repos.extend(data)
        if len(data) < 100:
            break
        page += 1
        await asyncio.sleep(0.5)
    return repos[:500]  # Limit to 500 most recent

# Clean up company name
def clean_company_name(company):
    if company:
        return company.strip().lstrip('@').upper()
    return ''

# Process and save user and repository data to CSVs
async def main():
    users = await get_users_in_chicago()
    all_user_data = []
    all_repo_data = []

    async with aiohttp.ClientSession(headers=HEADERS) as session:
        for user in users:
            username = user['login']
            user_details = await fetch(session, f'https://api.github.com/users/{username}')
            if not user_details:  # Skip if user details fetch fails
                continue

            repos = await get_user_repos(session, username)

            # Process user data
            all_user_data.append([
                user['login'],
                user_details.get('name', ''),
                clean_company_name(user_details.get('company', '')),
                user_details.get('location', ''),
                user_details.get('email', ''),
                str(user_details.get('hireable', '')).lower(),
                user_details.get('bio', ''),
                user_details.get('public_repos', 0),
                user_details.get('followers', 0),
                user_details.get('following', 0),
                user_details.get('created_at', '')
            ])

            # Process repository data
            for repo in repos:
                license_name = repo.get('license', {}).get('name') if repo.get('license') else ''
                all_repo_data.append([
                    user['login'],
                    repo['full_name'],
                    repo['created_at'],
                    repo['stargazers_count'],
                    repo['watchers_count'],
                    repo.get('language', ''),
                    str(repo.get('has_projects', False)).lower(),
                    str(repo.get('has_wiki', False)).lower(),
                    license_name
                ])

    # Save user data to users.csv
    with open('users.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['login', 'name', 'company', 'location', 'email', 'hireable', 'bio', 'public_repos', 'followers', 'following', 'created_at'])
        writer.writerows(all_user_data)

    # Save repository data to repositories.csv
    with open('repositories.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['login', 'full_name', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'has_projects', 'has_wiki', 'license_name'])
        writer.writerows(all_repo_data)

# Run the main function with compatibility for environments with an active event loop
try:
    # For environments that allow asyncio.run()
    asyncio.run(main())
except RuntimeError as e:
    # For Jupyter or environments with an active loop
    if str(e) == "asyncio.run() cannot be called from a running event loop":
        await main()
    else:
        raise

"""Q1"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/users.csv')

# Sort the users by followers in descending order and select the top 5
top_5_users = users_df.sort_values(by='followers', ascending=False).head(5)

# Get the login names of the top 5 users as a comma-separated string
top_5_logins = ','.join(top_5_users['login'].tolist())

print("Top 5 users by followers in Chicago:", top_5_logins)

"""Q2"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('users.csv')

# Convert the created_at column to datetime format for proper sorting
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Sort the users by created_at in ascending order and select the top 5
earliest_users = users_df.sort_values(by='created_at').head(5)

# Get the login names of the earliest 5 users as a comma-separated string
earliest_logins = ','.join(earliest_users['login'].tolist())

print("5 earliest registered GitHub users in Chicago:", earliest_logins)

"""q3"""

import pandas as pd

# Load the repositories.csv file
repos_df = pd.read_csv('/content/repositories.csv')

# Filter out rows with missing license_name
repos_df = repos_df[repos_df['license_name'].notna()]

# Count the occurrences of each license_name and select the top 3
top_3_licenses = repos_df['license_name'].value_counts().head(4).index.tolist()

# Convert the top 3 licenses to a comma-separated string
top_3_licenses_str = ','.join(top_3_licenses)

print("3 most popular licenses:", top_3_licenses_str)

"""q4"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/users.csv')

# Filter out rows with missing or empty company names
users_df = users_df[users_df['company'].notna() & (users_df['company'] != '')]

# Count occurrences of each company and get the one with the highest count
most_common_company = users_df['company'].value_counts().idxmax()

print("The company where the majority of developers work:", most_common_company)

"""q5"""

import pandas as pd

# Load the repositories.csv file
repos_df = pd.read_csv('/content/repositories.csv')

# Filter out rows with missing or empty language names
repos_df = repos_df[repos_df['language'].notna() & (repos_df['language'] != '')]

# Count occurrences of each language and get the most common one
most_popular_language = repos_df['language'].value_counts().idxmax()

print("The most popular programming language among these users:", most_popular_language)

repos_df['language'].value_counts()

"""q6"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/users.csv')

# Filter users who joined after 2020
users_df['created_at'] = pd.to_datetime(users_df['created_at'])  # Convert to datetime
filtered_users = users_df[users_df['created_at'] > '2020-01-01']

# Load the repositories.csv file
repos_df = pd.read_csv('/content/repositories.csv')

# Filter repositories for the filtered users
filtered_repos_df = repos_df[repos_df['login'].isin(filtered_users['login'])]

# Filter out rows with missing or empty language names
filtered_repos_df = filtered_repos_df[filtered_repos_df['language'].notna() & (filtered_repos_df['language'] != '')]

# Count occurrences of each language
language_counts = filtered_repos_df['language'].value_counts()

# Get the second most popular language
second_most_popular_language = language_counts.index[1] if len(language_counts) > 1 else None

if second_most_popular_language:
    print("The second most popular programming language among users who joined after 2020:", second_most_popular_language)
else:
    print("There are not enough programming languages to determine the second most popular.")

"""q7"""

import pandas as pd

# Load the repositories.csv file
repos_df = pd.read_csv('/content/repositories.csv')

# Filter out rows with missing or empty language names
repos_df = repos_df[repos_df['language'].notna() & (repos_df['language'] != '')]

# Calculate average stars per repository for each language
average_stars = repos_df.groupby('language')['stargazers_count'].mean().reset_index()

# Identify the language with the highest average stars
highest_average_language = average_stars.loc[average_stars['stargazers_count'].idxmax()]

print(f"The programming language with the highest average number of stars per repository is: {highest_average_language['language']} "
      f"with an average of {highest_average_language['stargazers_count']:.2f} stars.")

"""q8"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Calculate leader_strength
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Sort by leader_strength in descending order and get the top 5
top_leaders = users_df.sort_values(by='leader_strength', ascending=False).head(5)

# Extract the 'login' column and join as comma-separated string
top_leader_logins = ','.join(top_leaders['login'])

print(top_leader_logins)

"""q9"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Calculate the correlation between followers and public_repos
correlation = users_df['followers'].corr(users_df['public_repos'])

# Print the result rounded to 3 decimal places
print(f"{correlation:.3f}")

"""q10"""

import pandas as pd
from scipy.stats import linregress

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Perform linear regression with public_repos as the independent variable and followers as the dependent variable
slope, intercept, r_value, p_value, std_err = linregress(users_df['public_repos'], users_df['followers'])

# Print the slope rounded to 3 decimal places
print(f"{slope:.3f}")

"""q11"""

import pandas as pd

# Load the repositories.csv file
repos_df = pd.read_csv('/content/sample_data/repositories.csv')

# Convert has_projects and has_wiki to binary format if they are not already
repos_df['has_projects'] = repos_df['has_projects'].astype(int)
repos_df['has_wiki'] = repos_df['has_wiki'].astype(int)

# Calculate the correlation between has_projects and has_wiki
correlation = repos_df['has_projects'].corr(repos_df['has_wiki'])

# Print the result rounded to 3 decimal places
print(f"{correlation:.3f}")

"""q12"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Filter the data based on hireable status and calculate the averages
hireable_avg_following = users_df[users_df['hireable'] == 'true']['following'].mean()
non_hireable_avg_following = users_df[users_df['hireable'] != 'true']['following'].mean()

# Calculate the difference and round to 3 decimal places
difference = round(hireable_avg_following - non_hireable_avg_following, 3)
print(difference)

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Replace 'true' with `True` and `None` with `False`
users_df['hireable'] = users_df['hireable'].fillna('false').str.lower() == 'true'

# Calculate the average following for hireable and non-hireable users
average_following_hireable = users_df[users_df['hireable'] == True]['following'].mean()
average_following_non_hireable = users_df[users_df['hireable'] == False]['following'].mean()

# Calculate the difference and print the result rounded to 3 decimal places
difference = average_following_hireable - average_following_non_hireable
print(f"{difference:.3f}")

"""q13"""

import pandas as pd
import statsmodels.api as sm

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Create a new column for bio word count
users_df['bio_word_count'] = users_df['bio'].str.split().str.len()

# Filter out users without bios
filtered_df = users_df[users_df['bio_word_count'] > 0]

# Prepare the dependent and independent variables
X = filtered_df['bio_word_count']  # independent variable
y = filtered_df['followers']        # dependent variable

# Add a constant to the independent variable
X = sm.add_constant(X)

# Perform linear regression
model = sm.OLS(y, X).fit()

# Get the slope (coefficient for bio_word_count)
slope = model.params['bio_word_count']

# Print the result rounded to 3 decimal places
print(f"{slope:.3f}")

"""q14"""

import pandas as pd

# Load the repositories.csv file
repos_df = pd.read_csv('/content/sample_data/repositories.csv')

# Convert 'created_at' to datetime
repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])

# Filter for weekend days (Saturday and Sunday)
# In pandas, 5 is Saturday and 6 is Sunday
weekend_repos = repos_df[repos_df['created_at'].dt.dayofweek.isin([5, 6])]

# Count the number of weekend repositories created by each user
weekend_counts = weekend_repos['login'].value_counts()

# Get the top 5 users
top_5_weekend_users = weekend_counts.head(5)

# Print the top 5 user logins in order
top_5_user_logins = ', '.join(top_5_weekend_users.index)
print(top_5_user_logins)

"""q15"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Replace None with False in hireable column
users_df['hireable'] = users_df['hireable'].fillna(False)

# Calculate fractions
hireable_users = users_df[users_df['hireable'] == True]
non_hireable_users = users_df[users_df['hireable'] == False]

# Count users with email addresses
hireable_with_email = hireable_users['email'].notnull().sum()
non_hireable_with_email = non_hireable_users['email'].notnull().sum()

# Calculate total users in each category
total_hireable = len(hireable_users)
total_non_hireable = len(non_hireable_users)

# Calculate fractions
fraction_hireable = hireable_with_email / total_hireable if total_hireable > 0 else 0
fraction_non_hireable = non_hireable_with_email / total_non_hireable if total_non_hireable > 0 else 0

# Calculate the difference
difference = round(fraction_hireable - fraction_non_hireable, 3)

# Print the result
print(difference)

"""q16"""

import pandas as pd

# Load the users.csv file
users_df = pd.read_csv('/content/sample_data/users.csv')

# Drop rows where the 'name' is missing
users_df = users_df[users_df['name'].notna()]

# Extract surnames (last word in the name)
users_df['surname'] = users_df['name'].str.strip().str.split().str[-1]

# Count occurrences of each surname
surname_counts = users_df['surname'].value_counts()

# Find the maximum count
max_count = surname_counts.max()

# Get the most common surname(s)
most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()

# Sort the surnames alphabetically if there are ties
most_common_surnames.sort()

# Print the result as a comma-separated string
result = ', '.join(most_common_surnames)
print(result)

